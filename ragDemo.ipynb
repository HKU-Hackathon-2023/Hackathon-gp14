{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "### Import Required Libaried\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "import dotenv \n",
    "#load the environment variables of .env file\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Azure Credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the required credential for using Azure cognitive search\n",
    "search_endpoint = f\"https://{os.getenv('AZURE_SEARCH_SERVICE')}.search.windows.net/\"\n",
    "search_creds = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))\n",
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                                index_name=os.getenv(\"AZURE_SEARCH_INDEX\"),\n",
    "                                credential=search_creds)\n",
    "\n",
    "# Setup the required credential for using Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")     \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(text):\n",
    "    return openai.Embedding.create(engine=\"embedding\", input=text)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def nonewlines(s: str) -> str:\n",
    "    return s.replace(' ', ' ').replace('\\r', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using ChatGPT Through calling API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the query to what you want to ask chatGPT\n",
    "query = \"Where is the capital of India?\"\n",
    "\n",
    "messages = [\n",
    "    {'role' : 'user', 'content' : query }\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "    deployment_id=\"chat\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=messages, \n",
    "    temperature=0.7, \n",
    "    max_tokens=1024, \n",
    "    n=1)\n",
    "\n",
    "chat_content = chat_completion.choices[0].message.content\n",
    "chat_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the role of ChatGPT \n",
    "- Adding system message\n",
    "- Adding few shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of love, the poets oft do write  \n",
      "With words so sweet, so fair, so bright  \n",
      "Its power doth move the soul within  \n",
      "And doth make hearts beat as one.\n",
      "\n",
      "Love is a flame that doth burn bright  \n",
      "Its light doth guide us through the night  \n",
      "It kindles hope within the heart  \n",
      "And doth make all our fears depart.\n",
      "\n",
      "Oh, love, how sweet thy power doth flow  \n",
      "Like a river, it doth ebb and flow  \n",
      "It doth make the world a brighter place  \n",
      "And doth bring a smile to every face.\n",
      "\n",
      "So let us embrace love's gentle might  \n",
      "And let our hearts take flight  \n",
      "For in love, we find our truest selves  \n",
      "And in love, we find our truest wealth.\n"
     ]
    }
   ],
   "source": [
    "#change the query to what you want to ask chatGPT\n",
    "query = \"Help me write a poem about love.\"\n",
    "\n",
    "#change the systemMessage to how you want chatGPT to behave\n",
    "systemMessage = '''You are a Shakespearean writing assistant who speaks in a Shakespearean style. \n",
    "                    You help people come up with creative ideas and content like stories, poems, and songs that use Shakespearean style of writing style, including words like \"thou\" and \"hath”.\n",
    "                    Here are some example of Shakespeare's style:\n",
    "                    - Romeo, Romeo! Wherefore art thou Romeo?\n",
    "                    - Love looks not with the eyes, but with the mind; and therefore is winged Cupid painted blind.\n",
    "                    - Shall I compare thee to a summer's day? Thou art more lovely and more temperate.'''\n",
    "\n",
    "messages = [\n",
    "    {'role' : 'system', 'content' : systemMessage},\n",
    "    #change the content here to your example question\n",
    "    {'role' : 'user', 'content' : 'Please write a short text turning down an invitation to dinner.'},\n",
    "    #change the content here to your example answer\n",
    "    {'role' : 'assistant', 'content' : '''Dearest,\n",
    "                                        Regretfully, I must decline thy invitation.\n",
    "                                        Prior engagements call me hence. Apologies.'''},\n",
    "    {'role' : 'user', 'content' : query }\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "    deployment_id=\"chat\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=messages, \n",
    "    temperature=0.7, \n",
    "    max_tokens=1024, \n",
    "    n=1)\n",
    "\n",
    "chat_content = chat_completion.choices[0].message.content\n",
    "print(chat_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing a RAG model\n",
    "\n",
    "### Obtain related information using vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_Innovation_Wings-3.pdf:   In this group, we plan to pursue several interdisciplinary projects that will improve the ways we can study and teach about the human past. Our projects will include uses of augmented and virtual reality in recording, teaching, and presenting archaeological sites. We are also studying the use of machine learning and computer vision for several purposes to study visual datasets such as satellite images of ancient landscapes. We work with a field project that travels to Armenia each summer to excavate. We welcome anyone who would like to join our team, from any Faculty. We especially invite engineers and computer scientists, but we also have many other projects which anyone with an interest can join. Let’s learn about the human past together! Thank you! SIG – HKU Astar (formerly RoboMaster ICRA AI Challenge)  HKU Astar is a student group who participates in Robomaster University AI Challenge Competition. Robomaster University AI Challenge is an AI-based robotics competition for university students held by DJI and IEEE International Conference on Robotics and Automation.\n",
      "The_Innovation_Wings-4.pdf:  • Only registered members of Innovation Wing are permitted to access to its facilities, equipment and services with his/her HKU student/staff card. • Please go through the following three steps (namely Registration, Deposit Payment, and General Safety Training) before granting access to the Innovation Wing. Step 1. Registration  Eligibility:  • Undergraduate students and academic staff in the Faculty of Engineering.  • HKU undergraduate students, taught postgraduate students and research postgraduate students, academic staff, and technical staff who is a team member of an Innovation Wing affiliated student interest group (SIG) are  eligible for membership. • Undergraduate students and academic staff who is a member of the InnoHub programme organized by the Innovation Academy. • Registered members of Innovation Wing are permitted access to its facilities, equipment and services.  Eligible students will be added to the following Innovation Wing Moodle page where they can apply for membership of Innovation Wing. If you are eligible for Innovation Wing membership but yet to have access to the following Moodle, please send an inquiry \n",
      "The_Innovation_Wings-2.pdf:  The ultimate purpose of the project is to develop the skills, quality and team spirit of engineering students that can be beneficial in their future career. SIG – HKU Racing  Formula Student is a renowned educational engineering competition, combining practical engineering with soft skills including business planning and project management. It is a proving ground for students who want to create and change the world. Electrification of transportation system is here, the combination of electric powertrain and traditional mechanical system has made this competition attractive to lots of industries leading to companys’ attention and support. This competition jump starts our students’ knowledge and skills set for their future career. HKU Racing is the first team from Hong Kong to compete in Formula Student (European series). SIG – Quantum Bit Demonstrator  The world is currently in the midst of a second-quantum revolution, which will see the counter intuitive properties of quantum systems such as superposition and entanglement, being applied for commercial technologies such as quantum computing, quantum sensing, and quantum communications.\n"
     ]
    }
   ],
   "source": [
    "#change the query to what you want to ask the RAG model\n",
    "query = \"Who is the instructor of the course\"\n",
    "query_vector = compute_embedding(query)\n",
    "\n",
    "r = search_client.search(query, \n",
    "                        top=3, \n",
    "                        vector=query_vector, \n",
    "                        top_k=50, \n",
    "                        vector_fields=\"embedding\")\n",
    "\n",
    "results = [doc[\"sourcepage\"] + \": \" + nonewlines(doc[\"content\"]) for doc in r]\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the GPT model with query + information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't see any specific information about the instructor of a course in the provided sources.\n"
     ]
    }
   ],
   "source": [
    "#change the systemMessage to how you want chatGPT to behave\n",
    "systemMessage = \"\"\"AI Assistant that helps user to answer questions from sources provided. Be brief in your answers.\n",
    "                    Answer ONLY with the facts listed in the list of sources below. \n",
    "                    If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. \n",
    "                    Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. \n",
    "                    Use square brackets to reference the source, e.g. [info1.txt]. Don't combine sources, list each source separately, e.g. [info1.txt][info2.pdf].\n",
    "                \"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role' : 'system', 'content' : systemMessage},\n",
    "    {'role' : 'user', 'content' : query + \"   Source:\" + \" \".join(results)}\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "    deployment_id=\"chat\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=messages, \n",
    "    temperature=0.7, \n",
    "    max_tokens=1024, \n",
    "    n=1)\n",
    "\n",
    "chat_content = chat_completion.choices[0].message.content\n",
    "print(chat_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
